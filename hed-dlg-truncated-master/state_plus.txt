
# Ubuntu HRED model used in "A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues"
# by Serban et al. (2016).
def prototype_ubuntu_HRED():#可以看这个作为例子，写自己的state函数
    state = prototype_state()

    state['end_sym_utterance'] = '__eot__'

    state['unk_sym'] = 0 # Unknown word token <unk>
    state['eos_sym'] = 1 # end-of-utterance symbol </s>
    state['eod_sym'] = -1 # end-of-dialogue symbol </d>
    state['first_speaker_sym'] = -1 # first speaker symbol <first_speaker>
    state['second_speaker_sym'] = -1 # second speaker symbol <second_speaker>
    state['third_speaker_sym'] = -1 # third speaker symbol <third_speaker>
    state['minor_speaker_sym'] = -1 # minor speaker symbol <minor_speaker>
    state['voice_over_sym'] = -1 # voice over symbol <voice_over>
    state['off_screen_sym'] = -1 # off screen symbol <off_screen>
    state['pause_sym'] = -1 # pause symbol <pause>

    state['train_dialogues'] = "../UbuntuData/Training.dialogues.pkl"
    state['test_dialogues'] = "../UbuntuData/Test.dialogues.pkl"
    state['valid_dialogues'] = "../UbuntuData/Validation.dialogues.pkl"
    state['dictionary'] = "../UbuntuData/Dataset.dict.pkl"
    state['save_dir'] = "Output"

    state['max_grad_steps'] = 80

    state['valid_freq'] = 5000

    state['prefix'] = "UbuntuModel_"
    state['updater'] = 'adam'

    state['bidirectional_utterance_encoder'] = False
    state['deep_dialogue_input'] = True
    state['deep_out'] = True

    state['bs'] = 80

    state['reset_utterance_decoder_at_end_of_utterance'] = True
    state['reset_utterance_encoder_at_end_of_utterance'] = True
    state['utterance_decoder_gating'] = 'LSTM'

    state['lr'] = 0.0002

    state['qdim_encoder'] = 500
    state['qdim_decoder'] = 500
    state['sdim'] = 1000
    state['rankdim'] = 300

    return state


def prototype_NLPCC_HRED():
    '''
    参考prototype_Ubuntu_HRED作为模板编写
    :return: state:字典
    '''
    state = prototype_state()

    state['end_sym_utterance'] = '</s>'

    state['unk_sym'] = 0  # Unknown word token <unk>
    state['eos_sym'] = 1  # end-of-utterance symbol </s>
    state['eod_sym'] = -1  # end-of-dialogue symbol </d>
    state['first_speaker_sym'] = -1  # first speaker symbol <first_speaker>
    state['second_speaker_sym'] = -1  # second speaker symbol <second_speaker>
    state['third_speaker_sym'] = -1  # third speaker symbol <third_speaker>
    state['minor_speaker_sym'] = -1  # minor speaker symbol <minor_speaker>
    state['voice_over_sym'] = -1  # voice over symbol <voice_over>
    state['off_screen_sym'] = -1  # off screen symbol <off_screen>
    state['pause_sym'] = -1  # pause symbol <pause>

    state['train_dialogues'] = "../Data2/Training.dialogues.pkl"
    state['test_dialogues'] = "../Data2/Test.dialogues.pkl"
    state['valid_dialogues'] = "../Data2/Validation.dialogues.pkl"
    state['dictionary'] = "../Data2/Dataset.dict.pkl"
    state['save_dir'] = "Output2"

    state['train_dialogues'] = "../Data2/Example.train.pkl"
    state['test_dialogues'] = "../Data2/Example.test.pkl"
    state['valid_dialogues'] = "../Data2/Example.valid.pkl"
    state['dictionary'] = "../Data2/Dataset.dict.pkl"
    state['save_dir'] = "Output2"

    state['max_grad_steps'] = 80

    state['valid_freq'] = 5000

    state['prefix'] = "NLPCCModel_"
    state['updater'] = 'adam'

    state['bidirectional_utterance_encoder'] = False
    state['deep_dialogue_input'] = True
    state['deep_out'] = True

    state['bs'] = 80

    state['reset_utterance_decoder_at_end_of_utterance'] = True
    state['reset_utterance_encoder_at_end_of_utterance'] = True
    state['utterance_decoder_gating'] = 'LSTM'

    state['lr'] = 0.0002

    state['qdim_encoder'] = 500
    state['qdim_decoder'] = 500
    state['sdim'] = 1000
    state['rankdim'] = 300

    return state

# Ubuntu VHRED model used in "A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues"
# by Serban et al. (2016). Note, this model was pretrained as the HRED model with state 'prototype_ubuntu_HRED'!
def prototype_ubuntu_VHRED():
    state = prototype_state()

    state['end_sym_utterance'] = '__eot__'

    state['unk_sym'] = 0 # Unknown word token <unk>
    state['eos_sym'] = 1 # end-of-utterance symbol </s>
    state['eod_sym'] = -1 # end-of-dialogue symbol </d>
    state['first_speaker_sym'] = -1 # first speaker symbol <first_speaker>
    state['second_speaker_sym'] = -1 # second speaker symbol <second_speaker>
    state['third_speaker_sym'] = -1 # third speaker symbol <third_speaker>
    state['minor_speaker_sym'] = -1 # minor speaker symbol <minor_speaker>
    state['voice_over_sym'] = -1 # voice over symbol <voice_over>
    state['off_screen_sym'] = -1 # off screen symbol <off_screen>
    state['pause_sym'] = -1 # pause symbol <pause>

    state['train_dialogues'] = "../UbuntuData/Training.dialogues.pkl"
    state['test_dialogues'] = "../UbuntuData/Test.dialogues.pkl"
    state['valid_dialogues'] = "../UbuntuData/Validation.dialogues.pkl"
    state['dictionary'] = "../UbuntuData/Dataset.dict.pkl"
    state['save_dir'] = "Output"

    state['max_grad_steps'] = 80

    state['valid_freq'] = 5000

    state['prefix'] = "UbuntuModel_"
    state['updater'] = 'adam'

    state['bidirectional_utterance_encoder'] = False
    state['deep_dialogue_input'] = True
    state['deep_out'] = True

    state['bs'] = 80

    state['reset_utterance_decoder_at_end_of_utterance'] = True
    state['reset_utterance_encoder_at_end_of_utterance'] = True
    state['utterance_decoder_gating'] = 'LSTM'

    state['lr'] = 0.0002

    state['qdim_encoder'] = 500
    state['qdim_decoder'] = 500
    state['sdim'] = 1000
    state['rankdim'] = 300

    # Latent variable configuration
    state['add_latent_gaussian_per_utterance'] = True
    state['latent_gaussian_per_utterance_dim'] = 100
    state['scale_latent_variable_variances'] = 0.1
    state['condition_latent_variable_on_dialogue_encoder'] = True
    state['train_latent_gaussians_with_kl_divergence_annealing'] = True
    state['kl_divergence_annealing_rate'] = 1.0/75000.0
    state['decoder_drop_previous_input_tokens'] = True
    state['decoder_drop_previous_input_tokens_rate'] = 0.75

    state['patience'] = 20

    return state

def prototype_NLPCC_VHRED():
    state = prototype_state()

    state['end_sym_utterance'] = '</s>'

    state['unk_sym'] = 0 # Unknown word token <unk>
    state['eos_sym'] = 1 # end-of-utterance symbol </s>
    state['eod_sym'] = -1 # end-of-dialogue symbol </d>
    state['first_speaker_sym'] = -1 # first speaker symbol <first_speaker>
    state['second_speaker_sym'] = -1 # second speaker symbol <second_speaker>
    state['third_speaker_sym'] = -1 # third speaker symbol <third_speaker>
    state['minor_speaker_sym'] = -1 # minor speaker symbol <minor_speaker>
    state['voice_over_sym'] = -1 # voice over symbol <voice_over>
    state['off_screen_sym'] = -1 # off screen symbol <off_screen>
    state['pause_sym'] = -1 # pause symbol <pause>

    state['train_dialogues'] = "../Data2/Training.dialogues.pkl"
    state['test_dialogues'] = "../Data2/Test.dialogues.pkl"
    state['valid_dialogues'] = "../Data2/Validation.dialogues.pkl"
    state['dictionary'] = "../Data2/Dataset.dict.pkl"
    state['save_dir'] = "Output2"


    #state['train_dialogues'] = "../Data2/Example_Training.dialogues.pkl"
    #state['test_dialogues'] = "../Data2/Example_Test.dialogues.pkl"
    #state['valid_dialogues'] = "../Data2/Example_Validation.dialogues.pkl"
    #state['dictionary'] = "../Data2/Dataset.dict.pkl"
    #state['save_dir'] = "Output2"

    state['max_grad_steps'] = 80

    state['valid_freq'] = 5000

    state['prefix'] = "NLPCCModel_"
    state['updater'] = 'adam'

    state['bidirectional_utterance_encoder'] = False
    state['deep_dialogue_input'] = True
    state['deep_out'] = True

    state['bs'] = 80

    state['reset_utterance_decoder_at_end_of_utterance'] = True
    state['reset_utterance_encoder_at_end_of_utterance'] = True
    state['utterance_decoder_gating'] = 'LSTM'

    state['lr'] = 0.0002

    state['qdim_encoder'] = 500
    state['qdim_decoder'] = 500
    state['sdim'] = 1000
    state['rankdim'] = 300

    # Latent variable configuration
    state['add_latent_gaussian_per_utterance'] = True
    state['latent_gaussian_per_utterance_dim'] = 100
    state['scale_latent_variable_variances'] = 0.1
    state['condition_latent_variable_on_dialogue_encoder'] = True
    state['train_latent_gaussians_with_kl_divergence_annealing'] = True
    state['kl_divergence_annealing_rate'] = 1.0/75000.0
    state['decoder_drop_previous_input_tokens'] = True
    state['decoder_drop_previous_input_tokens_rate'] = 0.75

    state['patience'] = 20



    return state
